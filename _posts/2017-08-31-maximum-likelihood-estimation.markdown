---
layout: post
title: Maximum Likelihood Estimation
date: '2017-08-31 06:00:00 -0600'
categories: machine-learning
tags:
  - machine-learning
  - statistics
---


To understand what a maximum likelihood estimation first we need to understand the
concept of an estimator. In statistics, an estimator often called **point estimation** is used for calculating an
estimate of the given quantity $$ \theta $$. Given an independent and identically distributed data
$$ \left\{ x_1, x_2, ... , x_m \right\} $$, point estimation $$ \hat\theta $$ of an [estimand](https://en.wikipedia.org/wiki/Estimand) $$ \theta $$ can be formally defined as:

$$

  \hat\theta = g(x_1, x_2, ..., x_m)

$$

It is any function *g* that takes an input data and gives an estimate of $$ \theta $$. The function
$$ g $$ is very general and does not have any restriction on it. If the function $$ g $$ returns a
function then the estimator is called **Function Estimator**. If suppose we know a function $$ f(x) $$
that describes a relation between $$ x $$ and output $$ y $$ then function estimator of *f* is any estimator
that approximates this function. Linear regression is a good example of a function estimator.

To determine if an estimator is a good estimator we have a few properties associated with it like:

  * Variance
  * Bias
  * Consistency

The goal of a machine learning algorithm is to find a an estimator that has good properties associated with it (low bias, low variance and consistent are some good properties for an estimator).
But it is difficult to find estimators that has good properties. Instead of defining an estimator and checking
whether it has good properties, it is better if a general principle for deriving these good estimators. There are many such
principles in statistics and one such principle is the Maximum likelihood estimation.

### Maximum Likelihood Estimation

Let $$ X = \left\{x_1, x_2, ... , x_m\right\} $$ be a identical and independently distributed sample data drawn from a
unknown distribution given by $$ p_{data}(x) $$ and let $$ p_{model}(y | x,\theta) $$  be a parametric model that defines a probability distribution of y for a specific value of $$ \theta $$.

The goal is to give a good estimator for $$\theta$$. In **Maximum Likelihood Estimation (MLE)** the estimator $$\hat\theta$$ is defined as:

$$
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}

\begin{align}

\hat\theta &= \argmax_{\theta} p_{model}(X,\theta) \\
           &= \argmax_{\theta} \prod_{i=1}^{m} p_{model}(x_i;\theta)

\end{align}
$$

This estimator maximizes the likelihood that the samples will be generated by the model defined by $$\theta$$.
Since the product of probability will cause numerical underflow, we take the log of the estimator. The log does not
affect the argmax function and hence maintains the equality

$$
\begin{align}

\hat\theta &= log ( \argmax_{\theta} p_{model}(X,\theta) )\\
           &= \argmax_{\theta} \sum_{i=1}^{m} log ( p_{model}(x_i;\theta) )

\end{align}
$$

Because the argmax does not change when we scale it, we divide by *m*.

$$
\begin{align}

\hat\theta &= \argmax_{\theta} (1 / m) * \sum_{i=1}^{m} log ( p_{model}(x_i;\theta) ) \\
           &= \argmax_{\theta} E_{x \sim p_{data}}[log ( p_{model}(x;\theta))]

\end{align}
$$

The Expection is with respect to the empirical distribution $$ p_{data} $$. We can also
express this in minimization form:

$$
\hat\theta = - \argmin_{\theta} E_{x \sim p_{data}}[log ( p_{model}(x;\theta))]
$$

An estimator that is derived from Maximum Likelihood Principle will have the important
property of **[consistency](https://en.wikipedia.org/wiki/Consistency_of_an_estimator)**.



### References

* [https://en.wikipedia.org/wiki/Estimator](https://en.wikipedia.org/wiki/Estimator)
* [https://en.wikipedia.org/wiki/Maximum_likelihood_estimation](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation)
